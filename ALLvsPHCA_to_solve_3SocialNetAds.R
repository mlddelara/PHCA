# Persistent Homology Classification Algorithm
#Required Libraries
library(TDA)
library(rgl)
library(dplyr)
library(dplyr)
library(caTools)
library(caret)

#General Parameters

maxd <- 1 #maxdimension
maxsc <- 1 #maxscale
SR <- 0.8 #Split Ratio Between training and testing


##########################################################################
## CALLING the SOCIAL NETWORK ADS DATA
## This part can be modified depending on which data you want to analyze.
##########################################################################
social <- read.csv("~/OneDrive - University of the Philippines/DATAforR/social/social.csv")

dataset = social[3:5]

######################
# FORMATTING DATASET #
######################
XY <- dataset
nr <- nrow(XY)
nc <- ncol(XY)
X <- dataset[-nc] #removes target column
XY[,nc] <- XY[,nc] + 1
XY[,nc] <- as.numeric(XY[,nc]) #Turning nonnumeric data to numeric

# Feature Scaling
XY[-nc] = scale(XY[-nc])



# Encoding the target feature as factor 
XY[,nc] = factor(XY[,nc])
classes <- levels(XY[,nc])
numclasses <- as.numeric(classes)
maxclass <- max(numclasses)
len <- length(numclasses)

KofCV <- 5
CVnum <- matrix(0:KofCV-1, nrow=1, byrow=FALSE)
CCM <- list()

precision <- matrix(,KofCV,maxclass)
recall <- matrix(,KofCV,maxclass)
f1 <- matrix(,KofCV,maxclass)
MeanPrecision <- matrix(,1,maxclass)
MeanRecall <- matrix(,1,maxclass)
MeanF1 <- matrix(,1,maxclass)
for (CV in 0:(KofCV-1)){
  
#Splitting data into classes

trainM <- list()
testM <- list()
DiagTr <- list()

# Initializing matrices used
BB <- matrix(1:maxclass, nrow=1, byrow=FALSE)
CC <- matrix(1:maxclass, nrow=1, byrow=FALSE)
DD <- matrix(1:maxclass, nrow=1, byrow=FALSE)
EE <- matrix(1:(maxclass*3), nrow=maxclass, byrow=FALSE)
FF <- matrix(1:(maxclass*3), nrow=maxclass, byrow=FALSE)
GG <- matrix(1:(maxclass*(nc-1)), nrow=maxclass, byrow=FALSE)
LL <- matrix(1:maxclass, nrow=maxclass, byrow=FALSE)
nrtr <- matrix(1:maxclass, nrow=1, byrow=FALSE)
nctr <- matrix(1:maxclass, nrow=1, byrow=TRUE)
nrte <- matrix(1:maxclass, nrow=1, byrow=TRUE)
ncte <- matrix(1:maxclass, nrow=1, byrow=TRUE)
numofDiagTrrows <- matrix(1:maxclass, nrow=1, byrow=TRUE)
MM <- filter(XY, XY[,nc] == 100)
newdata <- MM
maxnum <- maxclass*3
DiagTrcolSums <- MM[1:3]
DiagTrcolmean <- MM[1:(nc-1)]
Featmean <- MM[1:(nc-1)]
FeatLength <- MM[1:1]


#######################
# PHCA Implementation #
#######################
for (class in numclasses)
{ 
  M <- filter(XY, XY[,nc] == class);
  #print(M);
  
  
  for (k in 1:nrow(M))
  {
    M[k,nc+1] <- k%%KofCV;
  }
  colnames(M)[nc+1] <- c("CVlabel")
  
  
  test_set = subset(M, CVlabel == CV)[-(nc+1)]
  training_set = subset(M, CVlabel != CV)[-(nc+1)]
  
  # Optional Feature Scaling, depending on dataset
  #training_set[-nc] = scale(training_set[-nc]) 
  #test_set[-nc] = scale(test_set[-nc])
  
  trainM[[class]] <- training_set
  testM[[class]] <- test_set
  newdata <- rbind(newdata, test_set)
  
  nrtr[class] <- nrow(training_set)
  nctr[class] <- ncol(training_set)
  nrte[class] <- nrow(test_set)
  ncte[class] <- ncol(test_set)
  print(c(nrtr[class], nctr[class], nrte[class], ncte[class]))
  
  ##################################
  # PH COMPUTATION of TRAINING SETS
  ##################################
  
  #####################################
  # Computing Rips persistence diagram 
  #####################################
  MMM <- trainM[[class]][1:nc-1]
  DiagTr[[class]] <- ripsDiag(X = MMM, maxdimension = maxd, maxscale = maxsc,
                              library = "GUDHI", printProgress = FALSE)
  par(mfrow=c(1,2))
  plot(DiagTr[[class]][["diagram"]], main = "Persistence Diagram")
  plot(DiagTr[[class]][["diagram"]], barcode = TRUE, main = "Persistence Barcode")
  numofDiagTrrows[class] <- nrow(DiagTr[[class]][["diagram"]])
  
  for (g in 1:3) {
    DiagTrcolmean[class,g] <- mean(DiagTr[[class]][["diagram"]][,g])
  }
  
  DiagTrcolSums <- rbind(DiagTrcolSums, colSums(DiagTr[[class]][["diagram"]]), deparse.level = 0)
  
  for (gg in 1:(nc-1)) {
    Featmean[class,gg] <- mean(MMM[,gg]) 
    
    #Lengthsum <- 0
    #or (i in nrow(DiagTr[[class]][["diagram"]])) {
    #  Lengthsum <- Lengthsum + DiagTr[[class]][["diagram"]][i,3] - DiagTr[[class]][["diagram"]][i,2]
    #}
    #FeatLength[class,] <- Lengthsum
  }
}

newdata        

# PHCA CLASSIFIER

resulta <- matrix(1:maxclass, nrow=1, byrow=FALSE)
result <- matrix(1:1, nrow=1, byrow=FALSE)


Classifying <- function(M5){
  
  for (j in numclasses) 
  {
    MM <- rbind(trainM[[j]], M5, deparse.level = 0)
    MM
    #countit <- countit + 1 #Optional
    MMtest <- MM[-nc]
    DiagTe <- ripsDiag(X = MMtest, maxdimension = maxd, maxscale = maxsc,
                       library = "GUDHI", printProgress = FALSE)
    
    BB[j] <- bottleneck(Diag1 = DiagTr[[j]][["diagram"]], 
                        Diag2 = DiagTe[["diagram"]],
                        dimension = 1)
    CC[j] <- wasserstein(Diag1 = DiagTr[[j]][["diagram"]], Diag2 = DiagTe[["diagram"]],
                         p = 2, dimension = 1)
    EE[j,1] <- abs(sum(DiagTe[["diagram"]][,1]) - DiagTrcolSums[j,1])
    EE[j,2] <- abs(sum(DiagTe[["diagram"]][,2]) - DiagTrcolSums[j,2])
    EE[j,3] <- abs(sum(DiagTe[["diagram"]][,3]) - DiagTrcolSums[j,3])
    FF[j,1] <- abs(mean(DiagTe[["diagram"]][,1]) - DiagTrcolmean[j,1])
    FF[j,2] <- abs(mean(DiagTe[["diagram"]][,2]) - DiagTrcolmean[j,2])
    FF[j,3] <- abs(mean(DiagTe[["diagram"]][,3]) - DiagTrcolmean[j,3])
    
    #Lengthsum <- 0
    #for (i in nrow(DiagTe[["diagram"]])) {
    #    Lengthsum <- Lengthsum + DiagTe[["diagram"]][i,3] - DiagTe[["diagram"]][i,2]
    #}
    #LL[j,1] <- abs(Lengthsum - FeatLength[j,])
    
    cc <- 0
    for (kk in 1:(nc-1)) {
      GG[j,kk] <- abs(Featmean[j,kk] - M5[1,kk])
      cc <- cc +GG[j,kk]
    }
    
    DD[j] <- -EE[j,1] + EE[j,3] - FF[j,1] + FF[j,3] + cc + CC[j]
  }
  resulta <- which( DD == min(DD), arr.ind=FALSE)
  
  print(resulta[1])
  return(resulta[1])
}

Count <- 0
for (ii in numclasses)
  
{
  for (jj in 1:nrte[ii])
  {
    Count <- Count + 1
    newdata[Count, nc+1] <- Classifying(testM[[ii]][jj,]) 
  }
}



# Results of predicting/classifying the testing set 
y_pred = newdata[,nc+1] #predict(classifier, newdata = test_set[-nc])

# Making the Confusion Matrix 
cm = table(newdata[, nc], y_pred)
confusionMatrix(cm)
CCM[[CV+1]] <- cm

####
# F1 score for multiclass classification problem
####
#y <- newdata[, nc] # factor of positive / negative cases
#predictions <- y_pred # factor of predictions

precision[(CV+1),] <- diag(cm) / colSums(cm)
recall[(CV+1),] <- diag(cm) / rowSums(cm)
}

f1 <- ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))

for (k in 1:maxclass){
  MeanPrecision[,k] <- mean(precision[,k])
}
for (k in 1:maxclass){
  MeanRecall[,k] <- mean(recall[,k])
}
for (k in 1:maxclass){
  MeanF1[,k] <- mean(f1[,k])
}




######################################
## OTHER CLASSIFICATION ALGORITHMS  ##
######################################
social <- read.csv("~/OneDrive - University of the Philippines/DATAforR/social/social.csv")

dataset = social[3:5]

dataset <- data.frame(social[3:5])
dataset[,nc] <- as.character(dataset[,nc])
data <- dataset
colnames(dataset) <- c("x1", "x2", "classname")

# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(dataset$classname , p=SR, list=FALSE)
# select 20% of the data for validation
validation <- dataset[-validation_index,]
# use the remaining 80% of data to training and testing the models
dataset <- dataset[validation_index,]

# dimensions of dataset
dim(dataset)

# list types for each attribute
sapply(dataset, class)
# take a peek at the first 5 rows of the data
head(dataset)
# list the levels for the class
dataset$classname = factor(dataset$classname)
levels(dataset$classname)

# summarize the class distribution
percentage <- prop.table(table(dataset$classname)) * 100
cbind(freq=table(dataset$classname), percentage=percentage)       

# summarize attribute distributions
summary(dataset)       

nc <- ncol(dataset)

# split input and output
x <- dataset[,1:(nc-1)]
y <- dataset[,nc]

# split input and output
x <- dataset[,1:(nc-1)]
y <- dataset[,nc]


# boxplot for each attribute on one image
par(mfrow=c(1,(nc-1)))
for(i in 1:(nc-1)) {
  boxplot(x[,i], main=names(dataset)[i])
}

#uninteresting
# barplot for class breakdown
plot(y)

# scatterplot matrix
featurePlot(x=x, y=y, plot="ellipse")

# box and whisker plots for each attribute
featurePlot(x=x, y=y, plot="box")

# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)


# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=5)
metric <- "Accuracy"

#VARIOUS MODELS

# a) linear algorithms
set.seed(7)
fit.lda <- train(classname~., data=dataset, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(classname~., data=dataset, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(classname~., data=dataset, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(classname~., data=dataset, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(classname~., data=dataset, method="rf", metric=metric, trControl=control)


# summarize accuracy of models #Selecting best model
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)


# compare accuracy of models
dotplot(results)

# estimate skill of LDA on the validation dataset
predictions <- predict(fit.lda, validation)
ypredlda = factor(validation[,nc])
confusionMatrix(predictions, ypredlda)[4] # validation$classname)
confusionMatrix(predictions, ypredlda)[2]

# estimate skill of CART on the validation dataset
predictions <- predict(fit.cart, validation)
ypredlda = factor(validation[,nc])
confusionMatrix(predictions, ypredlda)[4] # validation$classname)
confusionMatrix(predictions, ypredlda)[2]

# estimate skill of KNN on the validation dataset
predictions <- predict(fit.knn, validation)
ypredlda = factor(validation[,nc])
confusionMatrix(predictions, ypredlda)[4] # validation$classname)
confusionMatrix(predictions, ypredlda)[2]

# estimate skill of SVM on the validation dataset
predictions <- predict(fit.svm, validation)
ypredlda = factor(validation[,nc])
confusionMatrix(predictions, ypredlda)[4] # validation$classname)
confusionMatrix(predictions, ypredlda)[2]

# estimate skill of RF on the validation dataset
predictions <- predict(fit.rf, validation)
ypredlda = factor(validation[,nc])
confusionMatrix(predictions, ypredlda)[4] # validation$classname)
confusionMatrix(predictions, ypredlda)[2]

print(paste("Mean Recall for class", classes,round(100*MeanRecall,2), "%"), sep="")
print(paste("Mean F1-score for class", classes,round(100*MeanF1,2), "%"), sep="")
print(paste("Mean F1-score for class", classes,round(100*MeanF1,2), "%"), sep="")
print(MeanPrecision)
print(MeanRecall)
print(MeanF1)
